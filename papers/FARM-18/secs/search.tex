\section{Search}
\label{sec:search}

As the search space of possible DSP program is extremely large, our search procedures must be exceptionally efficient. 
As a first foray into DSP-PBE, we restrict ourselves to only synthesizing low-pass and high-pass filters, and global volume adjustment.
These two filters have the key property that they are quasi-commutative - when the thresholds of these filters do not overlap, applying a low-pass and then a high-pass is the same as applying a high-pass and then a low-pass.
We leave the exploration of non-commutative filters (for example, delay lines or ring filters) to future work.

\subsection{Gradient Descent}

Gradient descent is a technique commonly used in modelling and machine learning.
Given a cost function, which represents the disagreement between a proposed model and the actual data, gradient descent can be used efficiently to minimize the cost and generate the model of best fit.
One key restriction on the cost function is that it must be convex - this is because gradient descent will ``descend'' along the surface of the cost function, in each step following the steepest gradient.
We have carefully designed our aural distance function from Section~\ref{sec:distance} to be as close to convex as possible.

\begin{figure}[!h]
\includegraphics[width=\columnwidth]{figs/distCurves} 
\caption{The distance curves showing the convex-like shape of the aural distance function. Each curve is the $dist(\synthFilter(I),O)$, with $\synthFilter = lpf(x)$ where the value of $x$ is taken from the x-axis.}
\label{fig:distCurves}
\end{figure}

In order to visualize the convexity of our distance metric, we plot the distance between pairs of examples, and various possible DSP filters in Figure~\ref{fig:distCurves}.
Here we only visualize the distance curves in the dimension of the low-pass filter.
Notice that the curves exhibit a clear ``saddle'', which represents the minimum cost.
In the ideal case, gradient descent will find these points.
Note that we do not have these graphs available during synthesis - producing the entire graph as in Figure~\ref{fig:distCurves} is prohibitively expensive.

In Figure~\ref{fig:distCurves}, the last curve we plot is the distance between \texttt{cartoon-spring.wav} and \texttt{cartoon-spring-hpf1500.wav}, the same file with a high pass filter applied with a threshold of 1500 Hz.
Notice that as the threshold of the low-pass filter applied to the input example (\texttt{cartoon-spring.wav} increases, the distance to the output example decreases. 
This is because as a low-pass filter's threshold increases, it allows more and more frequencies to pass into the output - thereby having less of an effect.
Whereas in the case of the \texttt{cartoon-spring-hpf1500.wav}, the true filter is a high-pass filter, so the less we apply a low-pass filter, the closer we get to the correct filter.

\begin{figure}[!h]
\includegraphics[width=\columnwidth]{figs/distCurveZoom} 
\caption{Zooming in (1000 to 1500 Hz) on a portion of a curve from Figure~\ref{fig:distCurves}, we see the aural distance function is not perfectly convex on the micro scale.}
\label{fig:microDist}
\end{figure}

There are a number challenges with working with gradient descent in the aural DSP domain.
The first is that the domain can have a high number of dimensions to search.
In our implementation, we only explore a two DSP filters and volume adjustment, but this results in 5 dimensional space (each filter requires both a threshold value and an amplitude value for how much of the filter to apply).
To speed up gradient descent, we use stochastic gradient descent, so that in each step, we only move in $d<5$ number of dimensions.

Additionally, on the micro scale, the distance function is susceptible to noise and not entirely smooth, as shown in Figure~\ref{fig:microDist}.
In order to handle the micro scale variations, we use a periodic restart of the gradient descent.
This means that every $n$ rounds, as defined by the user (we use 4), the gradient descent will backtrack to the best solution it has found so far.
The stochastic gradient descent will then continue, selecting dimensions to explore in each round using a new random seed.

\begin{figure}[!h]
\includegraphics[width=\columnwidth]{figs/distCurveMacro} 
\caption{Looking at the portion of a curve from Figure~\ref{fig:distCurves} between 8k Hz and 20k Hz, we see the aural distance function is not perfectly convex on the macro scale. In this case, that is because the sample has very few frequencies above the 8k Hz range.}
\label{fig:macroDist}
\end{figure}

On the macro scale, we face the challenge that the distance function is only quasi-convex - there are many local minimum and long plateaus, as shown in Figure~\ref{fig:macroDist}.
In order to overcome this, we must carefully pick the initial value for gradient descent.
If we pick a value in the middle of a plateau, the gradient descent algorithm will not find any significant gradient, and conclude we have reached the convergence condition.
In our current implementation, we iterate at large intervals (1000 Hz) possible threshold values for both low and high pass filters.
We choose possible DSP programs that use only low pass, only high pass, and both low and high pass filters.
After evaluating these, we take the lowest cost initial DSP program, and start gradient descent from that point.

Finally, one of the key parts of a good application of gradient descent is the choice of the parameters such as the learning rate and the convergence goal.
These parameters must be adjusted based on the values observed from the cost (in our case, distance) function.
While the details of tuning gradient descent are outside the scope of this paper, it suffices to note that any change in the distance metric will likely also require a readjustment of these parameters.

