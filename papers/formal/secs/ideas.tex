\section{sketches}

Approximate programming-by-example combines reasoning-based program search with numerical search methods from machine learning.
The high-level idea is to shrink the search space as much as possible with formal methods, then switch over to machine learning.


\subsection{Preliminaries}
A \textit{Metric Space} is a pair $(M,d)$, where S is a set and $d:M \times M \to \reals$ is a distance metric between all points in the set. The metric $d$ must obey some properties that are listed on wikipedia...

\iffalse
I think I dont need this actually
A product metric $(X^{*},d_p)$ is a metric space over the Cartesian product of finitely many metric spaces $(X_0,d_0),...,(X_n,d_n)$,
  where $d_p:X^n \times X^n \to \reals$ also obey those properties.
If $d_p$ is a norm ``which is non-decreasing as the coordinates of a positive n-tuple increase``\footnote{took this from \url{https://en.wikipedia.org/wiki/Metric_space#Product_metric_spaces}, not sure how to formalize it yet}, then then topology of the X^* is equivelant
\fi

A \textit{grammar} $G = (N,\Sigma,P,S)$ is...
The language of a grammar $\languageOf{G} = \{ \}$ is the set of all words that can be generated by the grammar.

\subsection{Problem Statement}
At a high level, \textit{Approximate Programming by Example} (APBE) can be seen as a generalization of classic Programming by Example (PBE).
Specifically, APBE relaxes the correctness criteria of PBE.
Whereas PBE searches for a program that correctly maps every given input-output examples, APBE searches for a program that closely maps every given input-output example.
We then formally state APBE as the following problem.
\samepage{
Given:
\begin{itemize}
  \item A domain of input-output examples $M^{*}$, and a distance function $d:M^{*} \to M^{*} \to \reals$ such that $(M^{*},\costFxn)$ is a metric space
  \item A set of input,output examples $\{(i_0,o_0),...,(i_n,o_n)\}$ where $\forall 0 \leq j \leq n.\ i_j,o_j \in M^{*}$
  \item A grammar $G$ where words of the grammar can be interpreted as functions over the input-output example domain, $\forall w \in \languageOf{G}.\ w : M^{*} \to M^{*}$ \markk{should I introduce a special notation to interpret words of the grammar as functions? Something like $[w]$?}
\end{itemize}
Find a program $p \in G$ such that $\sum_{j=0}^{n} \ \costFxn(o_j,p(i_j))$ is minimized. 
\markk{Should I generalize the sum here? Just using sum might allow for some strange solutions}.
}

As an optimization problem, machine learning techniques such as gradient descent are an attractive option.
However, such methods operate by repeatedly computing the derivate of the distance function $\costFxn$.
If $\costFxn$ is not easily differentiable, for instance when $\costFxn$ uses blackbox I/O, exploring the complete space of $\languageOf{G}$ will be prohibitively expensive.
To overcome this challenge, we refine the search space using formal methods generate a $G'$ such that $\languageOf{G'} \subset \languageOf{G}$.

\section{Grammar Refinement}
To do this we also need a set of metric spaces $(M_0,d_0),...,(M_m,d_m)$, where the sets $M_0,..,M_m$ are built from the projections ($proj_i:M^{*} \to M_i$ into lower dimensional spaces (\markk{how to formalize this?}).

The user then provides a selection of dimension $i,j$ \markk{these don't need to be contiguous} and a grammar transformation $t:G \to G$ where $0 \leq i \leq j \leq m$, $\languageOf{t(G)} \subset \languageOf{G}$, where $N$ is the Euclidean norm,
\markk{Im not so sure about the >0 part above - but its something like that}

\begin{align*}
  N\big{(} d_i(proj_i(i),proj_i(o)) ,..., d_j(proj_j(i),proj_j(o)) \big{)} > 0 \\
\implies \\
\forall p \in G'.\ C(o,p(i)) > 0 \\
\end{align*}

This means, the user tells us if the input/output examples have some relation in the $[i,j]$ dimensions, then we can 'cut off' part of the search space.

\section{Gradient Descent-like optimization}

While gradient descent is an option, APBE is applied when $\costFxn$ is slow, so we should instead consider alternatives to gradient descent.
