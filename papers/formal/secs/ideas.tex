\section{sketches}

Approximate programming-by-example combines reasoning-based program search with numerical search methods from machine learning.
The high-level idea is to shrink the search space as much as possible with formal methods, then switch over to machine learning.


\subsection{Preliminaries}
A \textit{Metric Space} is a pair $(M,d)$, where S is a set and $d:M \times M \to \reals$ is a distance metric between all points in the set. The metric $d$ must obey some properties that are listed on wikipedia...

A \textit{grammar} $G = (N,\Sigma,P,S)$ is...
The language of a grammar $\languageOf{G} = \{ \}$ is the set of all words that can be generated by the grammar.

\markk{The work in~\cite{zhu2012randomized} uses a dataflow based model of computation. That model is too restricted for us, but maybe we can use something like that instead of grammars, which are very general?}

\subsection{Problem Statement}
At a high level, \textit{Approximate Programming by Example} (APBE) can be seen as a generalization of classic Programming by Example (PBE).
Specifically, APBE relaxes the correctness criteria of PBE.
Whereas PBE searches for a program that correctly maps every given input-output examples, APBE searches for a program that closely maps every given input-output example.
We then formally state APBE as the following optimization problem.

\noindent\textbf{Given:}
\begin{itemize}[topsep=0pt]
  \item A domain of input-output examples $\exampleDomain$, and a distance function $d:\exampleDomain \to \exampleDomain \to \reals$ such that $(\exampleDomain,\distFxn)$ is a metric space.
  \item A set of input,output examples $\{(i_0,o_0),...,(i_n,o_n)\}$ where $\forall 0 \leq j \leq n.\ i_j,o_j \in \exampleDomain$
  \item A grammar $G$ of functions over the input-output example domain, $\forall f \in \languageOf{G}.\ f: \exampleDomain \to \exampleDomain$
  \item A cost function $\costFxn : \languageOf{G} \to \reals$ that calculates how well a particular $p \in \languageOf{G}$ maps the input examples to the output examples. This is user-defined, but will generally utilize the distance function, $\distFxn$, for example in $\costFxn(p) = \sum_{j=0}^{n} (\distFxn(o_j,p(i_j)))$.
\end{itemize}
\textbf{Minimize:}

Find a program $p \in \languageOf{G}$ to minimize $\costFxn(p)$.
\vspace{\baselineskip}

There are a number of existing tools and techniques to solve such minimization/optimization problems~\cite{optmizationTextbook}.
However, one of the core components of such optimizations is the need for repeated computation of the cost function $\costFxn$ with candidate solutions, $p \in \languageOf{G}$.
When $c$ is particularly slow to compute, for instance when $\distFxn$ uses blackbox I/O, exploring the complete space of $\languageOf{G}$ is prohibitively expensive.
To overcome this challenge, we can proactively prune the search space before applying optimization techniques.
Since the optimization problem of APBE is over a space of programs, we can leverage techniques from formal methods to first refine the search space by generating a $G'$ such that $\languageOf{G'} \subset \languageOf{G}$.

\section{Grammar Refinement}
In order to generate a reduced grammar $G', \languageOf{G'} \subset \languageOf{G}$, we require domain-specific knowledge from the user.
Specifically, the user provides a grammar transformation $t:G \to G$ where $\languageOf{t(G)} \subset \languageOf{G}$ \markk{overloading G as the grammar and some kind of type, need to fix this - how do I write the 'type' of $G$}
  and a refinement predicate $r: \exampleDomain \to \exampleDomain \to Boolean$ that describes when the transformation $t$ can be applied. 

If the input/output examples have some relation $r$, then we can transform the search space in a way that removes potential programs that are not good.
Put formally:

\begin{align*}
   r(i,o) &\implies \\
   & \forall p \in \languageOf{G} \setminus \languageOf{t(G)}. \\
   & \exists p' \in \languageOf{t(G)}. \\
   & \distFxn(o,p(i)) > \distFxn(o,p'(i)) 
\end{align*}


\section{Gradient Descent-like optimization}

While gradient descent is an option, APBE is applied when $\distFxn$ is slow, so we should instead consider alternatives to gradient descent.


\section{Related}

The work of~\cite{misailovic2011probabilistically} introduces a notion of approximate correctness of program transformation over probabilistic programs.
Aside from our work focusing on synthesis, we also are working over deterministic programs - our notion of approximate is not with respect to probabilistic outcomes, but closeness in the metric space.

