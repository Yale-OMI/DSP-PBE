
\section{Problem Statement}

One of the core challenges in applying programming by example for DSP is to quantify how close our synthesized solution is to the correct solution.
In this case, the closeness of two audio files is a subjective measure - we do not necessarily know what features of the input/output audio pair the user is trying to emulate.
We use as the distance function, $\distFxn$, between two audio files, an audio fingerprinting technique~\cite{SantolucitoFARM} that approximates a measure of how different two audio clips sound to the human ear.
This metric is focused on measuring the amplitudes of frequency peaks obtains from a sliding window FFT.

\markk{add metric}

p
A \textit{metric space}~\cite{textbook} is a set with a structure that allows us to measure this sense of approximate.
A metric space is defined as a pair $(M,d)$, where S is a set and $d:M \times M \to \reals$ is a distance metric between all elements in the set. 
To form a metric space, the metric $d$ must obey the properties:

\begin{enumerate}
  \item $d(x,y) = 0 \Leftrightarrow x = y$ - identity of indiscernibles
  \item $d(x,y)  = d(y,x)$ - symmetry
  \item $d(x,z) \le d(x,y) + d(y, z)$ - triangle inequality
\end{enumerate}



This is not actually a metirc space. Here is an example of two audio files that violate the identity of indiscernibles property of a metric space.
However, this is a pathological examples and in most cases is close enough for us. 
We show that this is a metric space by running a set of tests of real world audio files

\subsection{Defining the search space}

In order to formalize the search space of DSP-PBE, we define a grammar of filters.
This grammar captures all allowable DSP programs that our synthesis technique will be able to find.

\begin{figure}
\begin{flalign*}
DSP := & \\
& |\ P \arrComp P\ \qquad & \text{\color{gray} sequential composition} & \\
& |\ P \parallelCompose P\ & \text{\color{gray} parallel composition} &\\
& |\ P & \\
P := & & \\
& |\ LPF \ [0,20k] \ [0,1] &\\
& |\ HPF \ [0,20k] \ [0,1] &\\
& |\ Ringz \ [0,20k] \ [0,1] &\\
& |\ PitchShift\ [0,20k] \ [0,1] &\\
& |\ WN \ [0,1] & 
\end{flalign*}
\caption{The grammar of DSP filters we consider}
\end{figure}


For a program $p \in \languageOf{G}$ generated from this grammar, we must fix constants for each parameterized node of the AST.
In this case, we must then choose $\constants :: \reals^{5}$ to generate an executable $p:\exampleDomain \to \exampleDomain$. 


\subsection{APBE as Optimization}
Note that this is distinct from a PBE setting with noisy examples - we cannot throw away examples that are anomalous and learn over the remaining examples~\cite{raychev2016learning}.
In \approximatePBE we want to find a program that fits as closely as possible to \textit{all} provided examples.
We additionally distinguish this problem from a general supervised learning problem, since in \approximatePBE, we have an additional constraint that the generated model must be a readable program.
We then formally state \approximatePBE as the following optimization problem.

\noindent\textbf{Given:}
\begin{itemize}[topsep=0pt]
  \item A domain of input-output examples $\exampleDomain$, and a distance function $d:\exampleDomain \to \exampleDomain \to \reals$ such that $(\exampleDomain,\distFxn)$ is a metric space.
  \item A user specified threshold $\epsilon \in \reals$ for minimal distance with respect to the metric space.
  \item A set of input,output examples $\{(i_0,o_0),...,(i_n,o_n)\}$ where $\forall 0 \leq j \leq n.\ i_j,o_j \in \exampleDomain$.
  \item A grammar $G$ of programs over the input-output example domain. 
        We notate the space of programs that can be generated from a grammar $G$ as $\languageOf{G}$.
        The grammar $G$ generates programs structures, that must be completed with constants $\constants$ - $\forall p \in \languageOf{G}.\ f: \constants \to \exampleDomain \to \exampleDomain$. 
        The program structure can be made into an function over the example space by fixing the constants, $\constants$, so that partial application of the program yields, $p(\constants): \exampleDomain \to \exampleDomain$. 
        The type of $\constants$ should be over a continuous space, for example $\reals^n$. 
  \item A cost function $\costFxn : \constants \to \languageOf{G} \to (\exampleDomain,\exampleDomain)^n \to \reals$ that calculates how well a particular $p(\constants) \in \languageOf{G}$ maps the $n$ input examples to the corresponding $n$ output examples. This is a user-defined error aggregation function of distance function, $\distFxn$, for example in $\costFxn(\constants,p,(i,o)) = \sum_{j=0}^{n} (\distFxn(o_j,p(\constants,i_j)))$.
\end{itemize}
\textbf{Minimize:}

Find a program $p \in \languageOf{G}$ and constants $\constants$ to minimize $\costFxn(\constants,p,(i,o))$. 
With respect to terminating synthesis, find $p$ and $k$ such that $\costFxn(\constants,p,(i,o)) \leq \epsilon$.
\vspace{\baselineskip}

There are a number of existing tools and techniques to solve general minimization/optimization problems over metric spaces~\cite{optmizationTextbook}.
However, one of the core components of such optimizations is the need for repeated computation of the cost function $\costFxn$ with candidate solutions, $p \in \languageOf{G}$.
When $c$ is particularly slow to compute, for instance when $\distFxn$ uses blackbox I/O, exploring the complete space of $\languageOf{G}$ is prohibitively expensive.
To overcome this challenge, we can proactively prune the search space before applying optimization techniques.
Since the optimization problem of \approximatePBE is over a space of programs, we can leverage techniques from formal methods to first refine the search space by generating a $G'$ such that $\languageOf{G'} \subset \languageOf{G}$.
