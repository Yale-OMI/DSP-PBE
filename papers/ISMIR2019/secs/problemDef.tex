
\section{Problem Statement}
From one perspective, \textit{Approximate Programming by Example} (\approximatePBE) can be seen as a generalization of classic Programming by Example (PBE).
Specifically, \approximatePBE relaxes the correctness criteria of PBE.
The problem statement of PBE is roughly to find a program that correctly maps every given input-output examples, and generalizes this mapping in a reasonable way across the input space, for a definition of reasonable that varies across application domains and search algorithms.
In contrast, \approximatePBE searches for a program that only \textit{closely} maps every given input-output example, and again generalizes the mapping across the input space.
The key difference is that in \approximatePBE, we do not require the examples must be mapped exactly.


\subsection{Measuring closeness}
One of the core aspect of \approximatePBE for DPS is in that we have some way of quantifying what approximate means.
A \textit{metric space}~\cite{textbook} is a set with a structure that allows us to measure this sense of approximate.
A metric space is defined as a pair $(M,d)$, where S is a set and $d:M \times M \to \reals$ is a distance metric between all elements in the set. 
To form a metric space, the metric $d$ must obey the properties:

\begin{enumerate}
  \item $d(x,y) = 0 \Leftrightarrow x = y$ - identity of indiscernibles
  \item $d(x,y)  = d(y,x)$ - symmetry
  \item $d(x,z) \le d(x,y) + d(y, z)$ - triangle inequality
\end{enumerate}

\subsection{Defining the search space}

We will use context-free, parameterized grammars to capture the types of programs we will synthesize.
The formalization of a grammar will allow us to describe how we refine how search space during the synthesis procedure.
A \textit{grammar} $G$ is defined as a tuple $(N,\Sigma,\productionRules,S)$.
The $\nonterminals$ and $\terminals$ denote sets of the nonterminal and terminal symbols of the grammar respectively.
A nonterminal $\nonterminals$ may be either a string constant, or a string constant parameterized with constants $\constants$ that are restricted to a range of continuous values $\reals$.
We write $X\ [a,b]^2 \ [c,d]$ to denote a nonterminal string constant $X$ with two constants over the range $[a,b]$ and one constant over the range $[c,d]$,

The set of production rules, $\productionRules$, define how an expression of terminals and nonterminals on the left-hand side are transformed into a new expression on the right-hand side, $LHS \to RHS$.
The left-hand side, $LHS = (\terminals \cup \nonterminals)^{*} \ \nonterminals \ \ (\terminals \cup \nonterminals)^{*}$, must contain at least one terminal, 
  and may be transformed on the right-hand side into any sequence of terminals and nonterminals $RHS = (\terminals \cup \nonterminals)^{*}$, including the empty string $\epsilon$.
The type of production rules can also be written in set notation $\productionRules :: \{(LHS,\{RHS\})\}$.
We use $proj_{LHS} :: P \to \{LHS\}$ as a projection function on the set of production rules.
Likewise, we write $proj_{RHS} :: LHS \to \{RHS\}$ as a projection function to extract the set of $RHS$ corresponding to a $LHS$ production rule.
The last part of the tuple, $S$ denotes the starting symbol of this generative grammar $G$.

For convenience of notation, we introduce the type symbol $\typeOfGrammar = (N,\Sigma,\productionRules,S)$ to denote the type of a grammar that is constructed as above.
The language of a grammar $\languageOf{G}$ is the set of all words that can be generated by the grammar $G$, where each word is the concatenation of some number of terminal symbols $\terminals^{*}$.
In our context of \approximatePBE, $\languageOf{G}$ is the set of all syntactically well-formed programs.


\begin{exmp}
\approximatePBE for Commutative Audio DSP filters.

In Audio DSP, the \exampleDomain is audio waveforms of type \texttt{[Double]}.
As an illustrative running example, we will choose a small grammar, $G$, of simple DSP filters, defined as follows:

\begin{align*}
DSP &= P \arrComp P\ |\ P \\
P   & = LPF \ [0,20k] \ [0,1] \ |\ HPF \ [0,20k] \ [0,1] \ |\ WN \ [0,1]
\end{align*}

For our metric space, we will use as the distance function $\distFxn$ an audio fingerprinting technique~\cite{SantolucitoFARM} that measures how different two audio clips sound to the human ear.

For a program $p \in \languageOf{G}$ generated from this grammar, we must fix constants for each parameterized node of the AST.
In this case, we must then choose $\constants :: \reals^{5}$ to generate an executable $p:\exampleDomain \to \exampleDomain$. 

\end{exmp}

\subsection{APBE as Optimization}
Note that this is distinct from a PBE setting with noisy examples - we cannot throw away examples that are anomalous and learn over the remaining examples~\cite{raychev2016learning}.
In \approximatePBE we want to find a program that fits as closely as possible to \textit{all} provided examples.
We additionally distinguish this problem from a general supervised learning problem, since in \approximatePBE, we have an additional constraint that the generated model must be a readable program.
We then formally state \approximatePBE as the following optimization problem.

\noindent\textbf{Given:}
\begin{itemize}[topsep=0pt]
  \item A domain of input-output examples $\exampleDomain$, and a distance function $d:\exampleDomain \to \exampleDomain \to \reals$ such that $(\exampleDomain,\distFxn)$ is a metric space.
  \item A user specified threshold $\epsilon \in \reals$ for minimal distance with respect to the metric space.
  \item A set of input,output examples $\{(i_0,o_0),...,(i_n,o_n)\}$ where $\forall 0 \leq j \leq n.\ i_j,o_j \in \exampleDomain$.
  \item A grammar $G$ of programs over the input-output example domain. 
        We notate the space of programs that can be generated from a grammar $G$ as $\languageOf{G}$.
        The grammar $G$ generates programs structures, that must be completed with constants $\constants$ - $\forall p \in \languageOf{G}.\ f: \constants \to \exampleDomain \to \exampleDomain$. 
        The program structure can be made into an function over the example space by fixing the constants, $\constants$, so that partial application of the program yields, $p(\constants): \exampleDomain \to \exampleDomain$. 
        The type of $\constants$ should be over a continuous space, for example $\reals^n$. 
  \item A cost function $\costFxn : \constants \to \languageOf{G} \to (\exampleDomain,\exampleDomain)^n \to \reals$ that calculates how well a particular $p(\constants) \in \languageOf{G}$ maps the $n$ input examples to the corresponding $n$ output examples. This is a user-defined error aggregation function of distance function, $\distFxn$, for example in $\costFxn(\constants,p,(i,o)) = \sum_{j=0}^{n} (\distFxn(o_j,p(\constants,i_j)))$.
\end{itemize}
\textbf{Minimize:}

Find a program $p \in \languageOf{G}$ and constants $\constants$ to minimize $\costFxn(\constants,p,(i,o))$. 
With respect to terminating synthesis, find $p$ and $k$ such that $\costFxn(\constants,p,(i,o)) \leq \epsilon$.
\vspace{\baselineskip}

There are a number of existing tools and techniques to solve general minimization/optimization problems over metric spaces~\cite{optmizationTextbook}.
However, one of the core components of such optimizations is the need for repeated computation of the cost function $\costFxn$ with candidate solutions, $p \in \languageOf{G}$.
When $c$ is particularly slow to compute, for instance when $\distFxn$ uses blackbox I/O, exploring the complete space of $\languageOf{G}$ is prohibitively expensive.
To overcome this challenge, we can proactively prune the search space before applying optimization techniques.
Since the optimization problem of \approximatePBE is over a space of programs, we can leverage techniques from formal methods to first refine the search space by generating a $G'$ such that $\languageOf{G'} \subset \languageOf{G}$.
