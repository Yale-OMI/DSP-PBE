\section{Metrical Synthesis}
\label{sec:opt}

In the parameter tuning phase of the algorithm we can leverage existing optimization techniques to find the best constants $\constants$ for a given program structure $p$.
One such optimization technique is gradient descent.
If the $\distFxn$ is not differentiable, for example using I/O, gradient descent may not be an appropriate option.
This stage can use any parameter tuning approach in place of gradient descent.

\subsection{Deriving Metric weights}
Additionally, we can improve our next parameter search over a new program structure by learning from synthesis attempts over other program structures.
To do this, we map the parameters from the best program in the previous parameter search onto the new program structure as an initial starting point for metrical synthesis.


\begin{exmp}
We take the same log from above, and see that, across all structures, \texttt{HPF 800} is slightly better than \texttt{200}, 
  and \texttt{LPF 200} is slightly better than \texttt{800}.
To leverage this new knowledge, we can accordingly adjust our initial values for the starting point of our gradient descent.
\end{exmp}
